{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage non supervisé des paramètres du HMM\n",
    "* Ne fonctionne pas mais dans l'idée le code est bon\n",
    "* Tous les calculs sont implementés et il y a des commentaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros, multiply\n",
    "import numpy as np\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import operator\n",
    "\n",
    "epsilon=1e-1\n",
    "\n",
    "class HMM:\n",
    "        def __init__(self, state_list, observation_list,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None, smoothing_obs = 0.01):\n",
    "            print \"HMM creating with: \"\n",
    "            self.N = len(state_list)       # number of states\n",
    "            self.M = len(observation_list) # number of possible emissions\n",
    "            print str(self.N)+\" states\"\n",
    "            print str(self.M)+\" observations\"\n",
    "            self.omega_Y = state_list\n",
    "            self.omega_X = observation_list\n",
    "            if transition_proba is None:\n",
    "                self.transition_proba = zeros( (self.N, self.N), float) \n",
    "            else:\n",
    "                self.transition_proba=transition_proba\n",
    "            if observation_proba is None:\n",
    "                self.observation_proba = zeros( (self.M, self.N), float) \n",
    "            else:\n",
    "                self.observation_proba=observation_proba\n",
    "            if initial_state_proba is None:\n",
    "                self.initial_state_proba = zeros( (self.N,), float ) \n",
    "            else:\n",
    "                self.initial_state_proba=initial_state_proba\n",
    "            self.make_indexes() # build indexes, i.e the mapping between token and int\n",
    "            self.smoothing_obs = smoothing_obs \n",
    "            \n",
    "        def make_indexes(self):\n",
    "            \"\"\"Creates the reverse table that maps states/observations names\n",
    "            to their index in the probabilities array\"\"\"\n",
    "            self.Y_index = {}\n",
    "            for i in range(self.N):\n",
    "                self.Y_index[self.omega_Y[i]] = i\n",
    "            self.X_index = {}\n",
    "            for i in range(self.M):\n",
    "                self.X_index[self.omega_X[i]] = i\n",
    "      \n",
    "        def get_observationIndices( self, observations ):\n",
    "            \"\"\"return observation indices, i.e \n",
    "            return [self.O_index[o] for o in observations]\n",
    "            and deals with OOVs\n",
    "            \"\"\"\n",
    "            indices = zeros( len(observations), int )\n",
    "            k = 0\n",
    "            for o in observations:\n",
    "                if o in self.X_index:\n",
    "                    indices[k] = self.X_index[o]\n",
    "                else:\n",
    "                    raise(\"Erreur\")\n",
    "                k += 1\n",
    "            return indices\n",
    "\n",
    "    \n",
    "        def data2indices(self, sent): \n",
    "            \"\"\"From one tagged sentence of the brown corpus: \n",
    "            - extract the words and tags \n",
    "            - returns two list of indices, one for each\n",
    "            -> (wordids, tagids)\n",
    "            \"\"\"\n",
    "            wordids = list()\n",
    "            tagids  = list()\n",
    "            for couple in sent:\n",
    "                wrd = couple[0]\n",
    "                tag = couple[1]\n",
    "                if wrd in self.X_index:\n",
    "                    wordids.append(self.X_index[wrd])\n",
    "                else:\n",
    "                    raise(\"Erreur data2indices\")\n",
    "                tagids.append(self.Y_index[tag])\n",
    "            return wordids,tagids\n",
    "            \n",
    "        def observation_estimation(self, pair_counts):\n",
    "            \"\"\" Build the observation distribution: \n",
    "                observation_proba is the observation probablility matrix\n",
    "                    [b_ki],  b_ki = Pr(X_t=v_k|Y_t=q_i)\"\"\"\n",
    "            # fill with counts\n",
    "            print(\"pair_counts\" , len(set(pair_counts.keys())))\n",
    "            for pair in pair_counts:\n",
    "                wrd=pair[0]\n",
    "                tag=pair[1]\n",
    "                cpt=pair_counts[pair]\n",
    "                k = 0 # for <unk>\n",
    "                if wrd in self.X_index: \n",
    "                    k=self.X_index[wrd]\n",
    "                i=self.Y_index[tag]\n",
    "                self.observation_proba[k,i]=cpt\n",
    "            # normalize\n",
    "            self.observation_proba=self.observation_proba+self.smoothing_obs\n",
    "            self.observation_proba=self.observation_proba/self.observation_proba.sum(axis=0).reshape(1,self.N)\n",
    "            \n",
    "        \n",
    "        def transition_estimation(self, trans_counts):\n",
    "            \"\"\" Build the transition distribution: \n",
    "                transition_proba is the transition matrix with : \n",
    "                [a_ij] a[i,j] = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            \"\"\"\n",
    "            # fill with counts\n",
    "            for pair in trans_counts:\n",
    "                i=self.Y_index[pair[1]]\n",
    "                j=self.Y_index[pair[0]]\n",
    "                self.transition_proba[j,i]=trans_counts[pair]\n",
    "            # normalize\n",
    "            self.transition_proba=self.transition_proba/self.transition_proba.sum(axis=0).reshape(1,self.N)\n",
    "        \n",
    "        def init_estimation(self, init_counts):\n",
    "            \"\"\"Build the init. distribution\"\"\"\n",
    "            # fill with counts\n",
    "            for tag in init_counts:\n",
    "                i=self.Y_index[tag]\n",
    "                self.initial_state_proba[i]=init_counts[tag]\n",
    "            # normalize\n",
    "            self.initial_state_proba=self.initial_state_proba/sum(self.initial_state_proba)\n",
    "             \n",
    "        \n",
    "        def supervised_training(self, pair_counts, trans_counts,init_counts):\n",
    "            \"\"\" Train the HMM's parameters. This function wraps everything\"\"\"\n",
    "            self.observation_estimation(pair_counts)\n",
    "            self.transition_estimation(trans_counts)\n",
    "            self.init_estimation(init_counts)\n",
    "        \n",
    "        def viverbit(self,mots):\n",
    "            alpha = np.zeros((self.N,len(mots)))\n",
    "            xi = np.zeros((self.N,len(mots)))\n",
    "            #init\n",
    "            #print(self.observation_estimation)\n",
    "            i = 0\n",
    "            if mots[0] in self.X_index:\n",
    "                i = self.X_index[mots[0]]\n",
    "            alpha[:,0] = self.initial_state_proba*self.observation_proba[i]\n",
    "            for i in range(1,len(mots)):\n",
    "                for j in range(self.N):\n",
    "                    #self.observation_proba : mot puis j : type\n",
    "                    indi = 0\n",
    "                    if mots[i] in self.X_index:\n",
    "                        indi = self.X_index[mots[i]]\n",
    "                    liste = [alpha[k,i-1] * self.transition_proba[k,j]* self.observation_proba[indi,j] for k in range(self.N)]\n",
    "                    alpha[j,i] = np.max(liste)\n",
    "                    xi[j,i] = np.argmax(liste)\n",
    "            tags = []\n",
    "            debut = len(xi)\n",
    "            starting = np.argmax(alpha[:,len(xi[0])-1])\n",
    "            tags.append(starting)\n",
    "            count = len(xi[0])\n",
    "            '''print(alpha)\n",
    "            print(xi)'''\n",
    "            while len(tags) != len(mots):\n",
    "                count -= 1\n",
    "                new_index = xi[starting,count]\n",
    "                tags.append(new_index)\n",
    "                starting = new_index\n",
    "            tags = tags[::-1]\n",
    "            to_return  = []\n",
    "            count = 0\n",
    "            for i in tags:\n",
    "                to_return.append((mots[count],self.Y_index.keys()[self.Y_index.values().index(i)]))\n",
    "                count +=1\n",
    "            return to_return\n",
    "                 \n",
    "        def evaluate(self,test_data):\n",
    "            errors = 0\n",
    "            total = 0\n",
    "            erreur_false = 0\n",
    "            total_false = 0\n",
    "            erreur_2 = 0\n",
    "            \n",
    "            correction = 0\n",
    "            correction_totale = 0\n",
    "            for i in range(len(test_data)):\n",
    "                res = self.viverbit(map(operator.itemgetter(0), test_data[i]))\n",
    "                if sum([a!=b for a,b in test_data[i]])>0:\n",
    "                    total_false +=1 \n",
    "                    erreur_false += sum([a[1]!=b[1] for a,b in zip(res,test_data[i])])\n",
    "                for a,b in zip(res,test_data[i]):\n",
    "                    if(b[0] != b[1] and b[1]==a[1]):\n",
    "                        correction +=1\n",
    "                    correction_totale +=1\n",
    "                erreur_2 += sum([a!=b for a,b in test_data[i]])\n",
    "                errors += sum([a[1]!=b[1] for a,b in zip(res,test_data[i])])\n",
    "                total += len(res)\n",
    "            print(\"Percentage of errors : \" ,  (errors/float(total))*100.0)\n",
    "            print(\"Pourcentage de correction  : \", (erreur_false/float(total_false)))\n",
    "            print(\"Taux d erreur brut \" , ((erreur_2/float(total))*100.0))\n",
    "            print(total)\n",
    "            print(\"taux correction calcul  2 \" , correction, ((correction/float(correction_totale))*100.0),correction_totale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compter les mots et les tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_counts(corpus):\n",
    "    \"\"\" \n",
    "    Build different count tables to train a HMM. Each count table is a dictionnary. \n",
    "    Returns: \n",
    "    * c_words: word counts\n",
    "    * c_tags: tag counts\n",
    "    * c_pairs: count of pairs (word,tag)\n",
    "    * c_transitions: count of tag bigram \n",
    "    * c_inits: count of tag found in the first position\n",
    "    \"\"\"\n",
    "    c_words = dict()\n",
    "    c_tags = dict()\n",
    "    c_pairs= dict()\n",
    "    c_transitions = dict()\n",
    "    c_inits = dict()\n",
    "    for sent in corpus:\n",
    "        # we use i because of the transition counts\n",
    "        for i in range(len(sent)):\n",
    "            couple=sent[i]\n",
    "            wrd = couple[0]\n",
    "            tag = couple[1]\n",
    "            # word counts\n",
    "            if wrd in c_words:\n",
    "                c_words[wrd]=c_words[wrd]+1\n",
    "            else:\n",
    "                c_words[wrd]=1\n",
    "            # tag counts\n",
    "            if tag in c_tags:\n",
    "                c_tags[tag]=c_tags[tag]+1\n",
    "            else:\n",
    "                c_tags[tag]=1\n",
    "            # observation counts\n",
    "            if couple in c_pairs:\n",
    "                c_pairs[couple]=c_pairs[couple]+1\n",
    "            else:\n",
    "                c_pairs[couple]=1\n",
    "            # i >  0 -> transition counts\n",
    "            if i > 0:\n",
    "                trans = (sent[i-1][1],tag)\n",
    "                if trans in c_transitions:\n",
    "                    c_transitions[trans]=c_transitions[trans]+1\n",
    "                else:\n",
    "                    c_transitions[trans]=1\n",
    "            # i == 0 -> counts for initial states\n",
    "            else:\n",
    "                if tag in c_inits:\n",
    "                    c_inits[tag]=c_inits[tag]+1\n",
    "                else:\n",
    "                    c_inits[tag]=1\n",
    "                    \n",
    "    return c_words,c_tags,c_pairs, c_transitions, c_inits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du vocabulaire (filtrage selon le nombre d'occurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_vocab(c_words, threshold):\n",
    "    \"\"\" \n",
    "    return a vocabulary by thresholding word counts. \n",
    "    inputs: \n",
    "    * c_words : a dictionnary that maps word to its counts\n",
    "    * threshold: count must be >= to the threshold to be included\n",
    "    \n",
    "    returns: \n",
    "    * a word list\n",
    "    \"\"\"\n",
    "    voc = list()\n",
    "    for w in c_words:\n",
    "        if c_words[w] >= threshold:\n",
    "            voc.append(w)\n",
    "    return voc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# les données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de phrases de train = 29057\n",
      "Nombre de phrases de test  = 1501\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "\n",
    "with open(\"typos-data/test10.pkl\", \"rb\") as input_file:\n",
    "    test = cPickle.load(input_file)\n",
    "    \n",
    "with open(\"typos-data/train10.pkl\", \"rb\") as input_file:\n",
    "    train = cPickle.load(input_file)\n",
    "print \"Nombre de phrases de train = \"+str(len(train))\n",
    "print \"Nombre de phrases de test  = \"+str(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots  : 26\n",
      "Nombre de tags  : 26\n",
      "Nombre de paires: 127\n",
      "Nombre de trans : 403 / 144\n",
      "Nombre de init. : 25\n",
      "Vocabulaire :26\n"
     ]
    }
   ],
   "source": [
    "cwords,ctags,cpairs,ctrans,cinits = make_counts(train)\n",
    "print \"Nombre de mots  : \"+str(len(cwords))\n",
    "print \"Nombre de tags  : \"+str(len(ctags))\n",
    "print \"Nombre de paires: \"+str(len(cpairs))\n",
    "print \"Nombre de trans : \"+str(len(ctrans))+ \" / \"+ str(12*12)\n",
    "print \"Nombre de init. : \"+str(len(cinits))\n",
    "vocab = make_vocab(cwords,10)\n",
    "print \"Vocabulaire :\"+str(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "26 states\n",
      "26 observations\n",
      "('pair_counts', 127)\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(state_list=ctags.keys(), observation_list=vocab,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None,\n",
    "                 smoothing_obs = 1e-2)\n",
    "hmm.supervised_training(cpairs,ctrans,cinits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baum-Welch\n",
    "\n",
    "basé sur http://vision.gel.ulaval.ca/~parizeau/Publications/P971225.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "26 states\n",
      "26 observations\n",
      "[('r', 'f'), ('e', 's'), ('e', 'f'), ('l', 's'), ('j', 'f'), ('h', 's'), ('g', 'f'), ('s', 'e')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:157: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class unsupervised_hmm:\n",
    "    def __init__(self,train,test,state_list,observation_list):\n",
    "        #calcul du nombre de labels possibles\n",
    "        self.N = len(state_list)       # number of states\n",
    "        self.M = len(observation_list) # number of possible emissions\n",
    "        #A : proba de transition\n",
    "        #B : proba d'observation\n",
    "        #initialsiation alétaoire\n",
    "        pi,A,B = self.random_val(self.N,self.M)\n",
    "        \n",
    "        #critère de convergence\n",
    "        has_converged = False\n",
    "        nb_iters = 10\n",
    "        \n",
    "        #cree l'index des observations\n",
    "        self.make_indexes(observation_list)\n",
    "        \n",
    "        #supprime les mots de taille <= 1\n",
    "        train_2 = []\n",
    "        for t in train:\n",
    "            if len(t)>1:\n",
    "                train_2.append(t)\n",
    "        train = train_2\n",
    "        \n",
    "        \n",
    "        while not has_converged and nb_iters>0:          \n",
    "            #nouveaux paramètres du hmm\n",
    "            tmpP,tmpA,tmpB = self.EM(pi,A,B,train,self.N,self.M)\n",
    "            pi = tmpP\n",
    "            A = tmpA\n",
    "            B = tmpB\n",
    "            nb_iters -= 25\n",
    "        \n",
    "        #algorithme terminé, appelle le hmm pour tester avec les paramètres calculés par l'algorithme\n",
    "        hmm = HMM(state_list=ctags.keys(), observation_list=vocab,\n",
    "                 transition_proba = A,\n",
    "                 observation_proba = B,\n",
    "                 initial_state_proba = pi)\n",
    "        print(hmm.viverbit(map(operator.itemgetter(0), test[9])))\n",
    "        \n",
    "    \n",
    "    def make_indexes(self,omega_Y):\n",
    "        #associe a chaque label un unique identifiant\n",
    "        self.Y_index = {}\n",
    "        for i in range(self.N):\n",
    "            self.Y_index[omega_Y[i]] = i\n",
    "            \n",
    "    def EM(self,pi,X,Y,obsevrations,N,M):\n",
    "        #e phase : stimation \n",
    "        \n",
    "        \n",
    "        #pour chaque donnée d'apprentissage calcule :\n",
    "        #gamma : proba d'être dans l'état i à l'instant t\n",
    "        #xi : proba d'être dans l'état i  à l'instant t et dans l'état j à l'instant t+1 pour une séquence d'observations\n",
    "        #alpha : proba d'observé une sequence d'observation de taille t<=T lorsqu'on se trouve dans l'etat q\n",
    "        #beta :proba d'être dans l'état q en ayant observé une sequence d'observation de taille t (t<T)\n",
    "        my_gamma = []\n",
    "        my_xi = []\n",
    "        for i in range(len(obsevrations)):\n",
    "            obs = np.array(obsevrations[i])\n",
    "            alpha = self.forward(obs,N,M,pi,X,Y)\n",
    "            #calcul de beta\n",
    "            beta = self.backward(obs,N,M,pi,X,Y)\n",
    "            #calcul proba\n",
    "            gamma, xi = self.exceptation(obs,N,M,pi,X,Y,alpha,beta)\n",
    "            my_gamma.append(gamma)\n",
    "            my_xi.append(xi)\n",
    "        \n",
    "        #nouveaux paramètres\n",
    "        new_pi = np.zeros(N)\n",
    "        new_A = np.zeros((N, N))\n",
    "        new_B = np.zeros((N, M))\n",
    "        \n",
    "        #PHASE MAXIMISATION\n",
    "        #reestimation des paramètres\n",
    "        \n",
    "        #calcul proba initial\n",
    "        for k in range(len(my_gamma)):\n",
    "            gamma = my_gamma[k]\n",
    "            new_pi += gamma[:, 0]\n",
    "        new_pi = new_pi/len(my_gamma)\n",
    "        \n",
    "        \n",
    "        #transition\n",
    "        #mise à jour des probas d'observation et de transition\n",
    "        my_xi = np.array(my_xi)\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                tmp = 0\n",
    "                test = 0\n",
    "                #for k in range(len(my_xi)):\n",
    "                #    tmp += sum(my_xi[k][:-1, i, j])\n",
    "                for k in range(len(my_xi)):\n",
    "                    for t in range(len(my_xi[k])):\n",
    "                        tmp += my_xi[k][t][i][j]\n",
    "                        test += my_gamma[k][i][t]\n",
    "                new_A[i,j] = tmp/test\n",
    "        for i in range(N):\n",
    "            dessous = 0\n",
    "            for k in range(len(my_gamma)):\n",
    "                for t in range(len(my_xi[k])):\n",
    "                    dessous += my_gamma[k][i][t]\n",
    "            for j in range(M):\n",
    "                test = 0\n",
    "                for k in range(len(obsevrations)):\n",
    "                    obs = np.array(obsevrations[k])\n",
    "                    for tk in range(obs.shape[0]):\n",
    "                        if self.Y_index[obsevrations[k][tk][0]] == j:\n",
    "                            if k < len(my_gamma):\n",
    "                                test += np.sum(my_gamma[k][tk])\n",
    "                new_B[i,j] = test / dessous\n",
    "        return (new_pi,new_A,new_B)\n",
    "    \n",
    "    \n",
    "    #calcule de forward\n",
    "    def forward(self,observations, N , M , pi , A , B):\n",
    "        #A : proba transition\n",
    "        #B : proba classe\n",
    "        T = observations.shape[0]\n",
    "        alphas = np.zeros((N,T))\n",
    "        i = 0\n",
    "        if observations[0][0] in self.Y_index:\n",
    "            i = self.Y_index[observations[0][0]]\n",
    "        alphas[:,0] = pi*B[i]\n",
    "        for t in range(1,T):\n",
    "            for j in range(N):\n",
    "                accu = sum([alphas[k,t-1]*A[k,j] for k in range(N)])\n",
    "                alphas[j,t] = accu * B[self.Y_index[observations[t][0]],j]\n",
    "        return alphas\n",
    "    \n",
    "    #calcule du backward\n",
    "    def backward(self,observations, N , M , pi , A ,B):\n",
    "        #A : proba transition\n",
    "        #B : proba classe\n",
    "        T = observations.shape[0]\n",
    "        betas = np.zeros((N, T))\n",
    "        betas[:,T-1] = 1 \n",
    "        \"\"\" Backward Updates \"\"\"\n",
    "        for t in range(T-2 , -1, -1):\n",
    "            for i in range(N):\n",
    "                accu = sum([B[self.Y_index[observations[t+1][0]],j]*betas[j,t+1]*A[j,i]for j in range(N)])\n",
    "                betas[i,t] = accu \n",
    "        return betas\n",
    "    \n",
    "    \n",
    "    #calcul de gamma et xi\n",
    "    def exceptation(self,obs,N,M,pi,A,B,alpha,beta):\n",
    "        #joint event xi\n",
    "        T = obs.shape[0]\n",
    "        #xi : proba d etre a instant t dans etat si  et dans etat j a t+1\n",
    "        xi = np.zeros((T,N,N))\n",
    "        gamma  = np.zeros((N,T))\n",
    "        for t in range(T-1):\n",
    "            for i in range(N):\n",
    "                for j in range(N):\n",
    "                    above = (alpha[i,t] * A[i,j] * B[self.Y_index[obs[t+1][0]],j] * beta[j,t+1])\n",
    "                    xi[t,i,j] = above\n",
    "        for t in range(T):\n",
    "            for i  in range(N):\n",
    "                gamma[i,t] = sum([xi[t,i,j]for j in range(N)])\n",
    "        return gamma,xi\n",
    "    \n",
    "    #initialisation aléatoire des paramètres\n",
    "    def random_val(self,N,M):\n",
    "        X = np.zeros((N, N))\n",
    "        Y = np.zeros((N, M))\n",
    "        pi = np.zeros(N)\n",
    "        #initialisation aleatoire\n",
    "        pi = np.random.random(N)\n",
    "        #normalisation\n",
    "        pi = pi / pi.sum()\n",
    "        for i in range(N):\n",
    "            tmp = np.random.random(N)\n",
    "            X[:, i] = tmp / tmp.sum()\n",
    "            tmp = np.random.random(M)\n",
    "            Y[:, i] = tmp / tmp.sum()\n",
    "        return (pi,X,Y)\n",
    "\n",
    "print(\"Test fait sur 10 itérations et 5000 données d'apprentissage\")\n",
    "uhmm = unsupervised_hmm(train[:5000],test,ctags.keys(),vocab)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
